{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution window search algorithm keep it here just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwindow settings\\nwindow_width = 50 \\nwindow_height = 80 # Break image into 9 vertical layers since image height is 720\\nmargin = 100 # How much to slide left and right for searching\\n\\ndef window_mask(width, height, img_ref, center,level):\\n   output = np.zeros_like(img_ref)\\n   output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\\n   return output\\n\\ndef find_window_centroids(image, window_width, window_height, margin):\\n   \\n   window_centroids = [] # Store the (left,right) window centroid positions per level\\n   window = np.ones(window_width) # Create our window template that we will use for convolutions\\n   \\n   # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\\n   # and then np.convolve the vertical image slice with the window template \\n   \\n   # Sum quarter bottom of image to get slice, could use a different ratio\\n   l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\\n   l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\\n   r_sum = np.sum(image[int(3*image.shape[0]/4):,int(image.shape[1]/2):], axis=0)\\n   r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]/2)\\n   \\n   # Add what we found for the first layer\\n   window_centroids.append((l_center,r_center))\\n   \\n   # Go through each layer looking for max pixel locations\\n   for level in range(1,(int)(image.shape[0]/window_height)):\\n       # convolve the window into the vertical slice of the image\\n       image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),:], axis=0)\\n       conv_signal = np.convolve(window, image_layer)\\n       # Find the best left centroid by using past left center as a reference\\n       # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\\n       offset = window_width/2\\n       l_min_index = int(max(l_center+offset-margin,0))\\n       l_max_index = int(min(l_center+offset+margin,image.shape[1]))\\n       l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\\n       # Find the best right centroid by using past right center as a reference\\n       r_min_index = int(max(r_center+offset-margin,0))\\n       r_max_index = int(min(r_center+offset+margin,image.shape[1]))\\n       r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\\n       # Add what we found for that layer\\n       window_centroids.append((l_center,r_center))\\n\\n   return window_centroids\\n\\nwindow_centroids = find_window_centroids(pres, window_width, window_height, margin)\\n\\n# If we found any window centers\\nif len(window_centroids) > 0:\\n\\n   # Points used to draw all the left and right windows\\n   l_points = np.zeros_like(pres)\\n   r_points = np.zeros_like(pres)\\n\\n   # Go through each level and draw the windows \\t\\n   for level in range(0,len(window_centroids)):\\n       # Window_mask is a function to draw window areas\\n       l_mask = window_mask(window_width,window_height,pres,window_centroids[level][0],level)\\n       r_mask = window_mask(window_width,window_height,pres,window_centroids[level][1],level)\\n       # Add graphic points from window mask here to total pixels found \\n       l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\\n       r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\\n\\n   # Draw the results\\n   template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\\n   zero_channel = np.zeros_like(template) # create a zero color channel\\n   template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\\n   warpage= np.dstack((pres, pres, pres))*255 # making the original road pixels 3 color channels\\n   output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\\n\\n# If no window centers found, just display orginal road image\\nelse:\\n   output = np.array(cv2.merge((pres,pres,pres)),np.uint8)\\n\\n# Display the final results\\nplt.imshow(output)\\nplt.title('window fitting results')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    " window settings\n",
    "window_width = 50 \n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin):\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(image[int(3*image.shape[0]/4):,:int(image.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(image[int(3*image.shape[0]/4):,int(image.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(image.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(image.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(image[int(image.shape[0]-(level+1)*window_height):int(image.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,image.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,image.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "window_centroids = find_window_centroids(pres, window_width, window_height, margin)\n",
    "\n",
    "# If we found any window centers\n",
    "if len(window_centroids) > 0:\n",
    "\n",
    "    # Points used to draw all the left and right windows\n",
    "    l_points = np.zeros_like(pres)\n",
    "    r_points = np.zeros_like(pres)\n",
    "\n",
    "    # Go through each level and draw the windows \t\n",
    "    for level in range(0,len(window_centroids)):\n",
    "        # Window_mask is a function to draw window areas\n",
    "        l_mask = window_mask(window_width,window_height,pres,window_centroids[level][0],level)\n",
    "        r_mask = window_mask(window_width,window_height,pres,window_centroids[level][1],level)\n",
    "        # Add graphic points from window mask here to total pixels found \n",
    "        l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "        r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "    # Draw the results\n",
    "    template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "    zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "    template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "    warpage= np.dstack((pres, pres, pres))*255 # making the original road pixels 3 color channels\n",
    "    output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    " \n",
    "# If no window centers found, just display orginal road image\n",
    "else:\n",
    "    output = np.array(cv2.merge((pres,pres,pres)),np.uint8)\n",
    "\n",
    "# Display the final results\n",
    "plt.imshow(output)\n",
    "plt.title('window fitting results')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def unwarp(undist,warped, left_fitx, right_fitx, ploty,Minv):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    \n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    #print(pts_left)\n",
    "    #print(pts_right)\n",
    "    #print(pts)\n",
    "    vertices = np.array([[(left_fitx[0],ploty[0]),(left_fitx[0],ploty[0]), (right_fitx[-1],ploty[-1]),(right_fitx[-1],ploty[-1])]], dtype=np.int32)\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, vertices, (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    \n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (undist.shape[1], undist.shape[0]), flags=cv2.INTER_LINEAR) \n",
    "    # Combine the result with the original image\n",
    "    #cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "    #print(newwarp.shape[0],undist.shape[0])\n",
    "    #print(newwarp.shape[1],undist.shape[1])\n",
    "    #print(newwarp.shape[2]) #9\n",
    "    print(Minv)\n",
    "    #print(undist.shape[2]) #3\n",
    "    #newwarp=cv2.cvtColor(newwarp,cv2.COLOR_BGR2GRAY)\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    plt.imshow(result)\n",
    "    return result\n",
    "    \n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
